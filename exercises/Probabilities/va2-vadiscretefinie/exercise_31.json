{
    "script": "Soit $X$ une variable aléatoire discrète finie prenant la valeur $x_i$ avec probabilité $p_i$,\r\npour $i=1,\\dots,n$. On définit l'entropie de $X$ par :\r\n$$H(X)=-\\sum_{i=1}^n p_i\\ln(p_i)$$\r\navec la convention $x\\ln x=0$ si $x=0$ (ce qui correspond au prolongement par continuité en $0$ de la fonction $x\\mapsto x\\ln x$).Démontrer que $H(X)\\geq 0$.Démontrer que $H(X)=0$ si et seulement si $X$ est presque sûrement constante, c'est-à-dire s'il existe $i\\in \\{1,\\dots, n\\}$ tel que $p_i=1$.Vérifier que, pour tout $k=1,\\dots,n$, on a \r\n$$(-np_k)\\ln (np_k)\\leq 1-np_k$$\r\navec égalité si et seulement si $np_k=1$.En déduire que $H(X)\\leq \\ln n$.Démontrer que $H(X)=\\ln n$ si et seulement si $X$ est équidistribuée, ie si $p_i=1/n$ pour tout $i=1,\\dots,n$.",
    "hint": "Pour un calcul, c'est très simple...Idem!On pourra utiliser la concavité de la fonction $x\\mapsto -x\\ln x$.",
    "solution": "On remarque que $x\\ln(x)\\leq 0$ si $x\\in [0,1[$ ce qui assure que $H(X)\\geq 0$.Si $X$ est presque sûrement constante, on a $p_i=1$ pour un $i$ et $p_j=0$ pour $j \\neq i$.\r\n On en déduit que $H(X)=-1\\times\\ln(1)=0$.Réciproquement, si $H(X)=0$, alors la preuve de la question précédente implique que, pour tout $i=1,\\dots,n$, on doit avoir $p_i\\ln(p_i)=0$. Ceci implique $p_i=0$ ou $p_i=1$. Puisque la somme des $p_i$ doit être égale à $1$, un des $p_i$ est donc égal à 1, et tous les autres $p_j$ sont nuls : $X$ est presque sûrement constante.Posons pour $x\\in ]0,+\\infty[$, $g(x)=-x\\ln x-(1-x)$. $g$ est dérivable sur $]0,+\\infty[$, de dérivée\r\n$g'(x)=-\\ln x-1+1=-\\ln x$. Ainsi, $g$ est strictement croissante sur $]0,1[$ et strictement décroissante sur $]1,+\\infty[$. Puisque $g(1)=0$, on en déduit que $g(x)\\leq 0$ si $x>0$ avec égalité si et seulement si $x=1$, et donc $-x\\ln x\\leq 1-x$, avec égalité si et seulement si $x=1$.\r\nOn en déduit que, pour tout $k=1,\\dots,n$,\r\n$$(-np_k)\\ln (np_k)=-np_k\\ln(n)-np_k\\ln(p_k)\\leq 1-np_k$$\r\navec égalité si et seulement si $np_k=1$.On somme les inégalités précédentes, pour $k$ allant de $1$ à $n$ :\r\n$$-n\\ln n+nH(X)\\leq n-n=0.$$\r\nCeci prouve le résultat voulu.Si $H(X)=\\ln n$, toutes les inégalités précédentes doivent être des égalités. On en déduit que $(-np_k)\\ln (np_k)=1-np_k$ pour tout $k=1,\\dots,n$, c'est-à-dire, $p_k=1/n$. Ainsi, si $H(X)=\\ln n$, $X$ est équirépartie.Réciproquement, si $X$ est équirépartie, on a $p_i=1/n$ pour tout $i$. On en déduit\r\n $$H(X)=\\sum_{i=1}^n -\\frac{\\ln (1/n)}{n}=-\\ln(1/n)=\\ln (n).$$\r\n $H(X)$ mesure le désordre engendré par $X$. Lorsque $X$ est presque sûrement constante,\r\n son entropie est nulle (pas de désordre). Lorsque la variable est équidistribuée, le désordre\r\n est maximal et l'entropie aussi.",
    "lang": "french"
}